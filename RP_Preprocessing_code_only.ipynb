{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gylam/siads696-sprsum2024-team05/blob/main/RP_Preprocessing_code_only.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "662cf06c-e718-4338-8559-7cd668fc613b",
      "metadata": {
        "id": "662cf06c-e718-4338-8559-7cd668fc613b"
      },
      "outputs": [],
      "source": [
        "#stopwords\n",
        "def create_stopwords():\n",
        "    \"\"\"creating a comprehensive list with nltk stopwords, json-en, punctuation\"\"\"\n",
        "    #nltk stopwords\n",
        "    ext_stopwords = stopwords.words('english')\n",
        "\n",
        "    #json-en stopwords\n",
        "    json_stop = requests.get(\"https://raw.githubusercontent.com/6/stopwords-json/master/dist/en.json\").json()\n",
        "    ext_stopwords =  set(ext_stopwords + json_stop + punctuation.split())\n",
        "    return ext_stopwords\n",
        "\n",
        "#text preprocessing code - the main function is pos_lemmatizer that call support functions remove_urls, tokenizer and penn2morphy\n",
        "def remove_urls(text):\n",
        "    \"\"\"remove urls from the data\"\"\"\n",
        "    url_pattern = r'https?://\\S+|www\\.\\S+'\n",
        "    text_without_urls = re.sub(url_pattern, '', text)\n",
        "    return text_without_urls\n",
        "\n",
        "def tokenizer(text):\n",
        "    \"\"\"tokenize the extracted text and remove stopwords/punctuation etc.\"\"\"\n",
        "    return [token for token in nltk.word_tokenize(text) if token.lower() not in ext_stopwords and len(token) >2\n",
        "            and re.search('^[a-zA-Z]{3,}$', token)]\n",
        "\n",
        "def penn2morphy(penntag):\n",
        "    \"\"\"Converts Penn Treebank tags to WordNet per here: https://www.kaggle.com/code/alvations/basic-nlp-with-nltk\"\"\"\n",
        "    morphy_tag = {'NN':'n', 'JJ':'a', 'VB':'v', 'RB':'r'}\n",
        "    try: return morphy_tag[penntag[:2]]\n",
        "    except:return 'n' # if mapping isn't found, fall back to Noun.\n",
        "\n",
        "def pos_lemmatizer(text):\n",
        "    \"\"\"pos tagging and lemmatization\"\"\"\n",
        "\n",
        "    pos_tokens = nltk.pos_tag(tokenizer(remove_urls(text)))\n",
        "    # useful explanation of what the tags mean: https://stackoverflow.com/questions/15388831/what-are-all-possible-pos-tags-of-nltk\n",
        "    wnl = WordNetLemmatizer()\n",
        "    wnl_tokens = []\n",
        "    for token, tag in pos_tokens:\n",
        "        if tag in [\"NNP\", \"NNPS\", \"CD\"]: #Dropping proper nouns and numericals (CD)\n",
        "            pass\n",
        "        else:\n",
        "            wnl_tokens.append(wnl.lemmatize(token, pos = penn2morphy(tag)).lower()) #these are the tokens we want to use in the vectorizer\n",
        "\n",
        "    return \" \".join(wnl_tokens)\n",
        "\n",
        "ext_stopwords = create_stopwords()\n",
        "\n",
        "t0 = time.time()\n",
        "df[\"cleaned_text\"] = df[\"combined_text\"].apply(pos_lemmatizer)\n",
        "t1 = time.time()\n",
        "print(f\"time taken to preprocess data:{round((t1-t0)/60,2)}\")\n",
        "# df.to_pickle(\"dataset_clean.pkl\")\n",
        "# df = pd.read_pickle(\"/home/prabhur/dataset_clean.pkl\")\n",
        "print(df.shape)\n",
        "df.head()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}